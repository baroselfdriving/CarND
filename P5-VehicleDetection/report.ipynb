{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 5 - Vehicle Detection - Report. Vilas Chitrakaran. March 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "The objective is to develop a software pipeline to identify and track vehicles in a video stream obtained from a front-facing camera mounted on the dashboard of a car. A linear support vector machine (SVM) is trained to identify vehicles based on histogram-of-oriented-gradients (HOG) features extracted from 'car' and 'non-car' image sets. The trained SVM is then applied on the video stream to search and identify vehicles. The robustness of the identification pipeline is improved by temporal filtering techniques to discard false positives. \n",
    "\n",
    "The following video demonstrates the final result obtained from my implementation.\n",
    "\n",
    "__TODO__: Link to final video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Pipeline Development\n",
    "\n",
    "__TODO__: Give an overview of sequence of development\n",
    "\n",
    "The vehicle is instrumented with a front-facing video camera that provides an RGB video stream of the road ahead.\n",
    "\n",
    "__NOTE__: Why am I anal about memory usage by deleting training variables after use? Not able to go past training stage on my laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# Prepare list of image files used for training\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# read in image as RGB\n",
    "def read_image(filename):\n",
    "    image = cv2.imread(filename)\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "trainingImages = glob.glob('./training_set/**/*.png', recursive=True)\n",
    "carImageNames = []\n",
    "notCarImageNames = []\n",
    "\n",
    "for filename in trainingImages:\n",
    "    if 'non-vehicles' in filename:\n",
    "        notCarImageNames.append(filename)\n",
    "    else:\n",
    "        carImageNames.append(filename)\n",
    "\n",
    "print('Number of car images in the dataset:', len(carImageNames))\n",
    "print('Number of non-car images in the dataset:', len(notCarImageNames))\n",
    "\n",
    "# Select a random set of car and non-car images and show it\n",
    "fig,ax = plt.subplots(2,5)\n",
    "fig.suptitle('Sample of training images: top Row - cars, bottom row - not cars')\n",
    "for i in range(5):\n",
    "    ax[0,i].imshow(read_image(carImageNames[np.random.randint(0, len(carImageNames))]))\n",
    "    ax[0,i].axis('off')\n",
    "    \n",
    "    ax[1,i].imshow(read_image(notCarImageNames[np.random.randint(0, len(notCarImageNames))]))\n",
    "    ax[1,i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.show()\n",
    "\n",
    "# free up variables after use. I have limited resources on my machine\n",
    "del trainingImages\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction - Histogram of Oriented Gradients (HOG)\n",
    "Why use HOG? spatial features add robustness. yCrCb seems to work better than HSV in some test images\n",
    "\n",
    "Explain how (and identify where in your code) you extracted HOG features from the training images. Explain how you settled on your final choice of HOG parameters.\n",
    "\n",
    "we will hard code these parameters\n",
    "- colorspace = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "- orient = 9\n",
    "- pix_per_cell = 8\n",
    "- cell_per_block = 2\n",
    "- hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# Feature extraction\n",
    "\n",
    "from skimage.feature import hog\n",
    "   \n",
    "# Wrapper to apply a chosen color conversion to an RGB image\n",
    "def convert_color(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "\n",
    "def get_spatial_features(image):\n",
    "    size = 16 \n",
    "    return cv2.resize(image, (size, size)).ravel()\n",
    "\n",
    "def get_histogram_features(image):\n",
    "    numBins=16\n",
    "    binsRange=(0, 256)\n",
    "    features = []\n",
    "    for channel in range(image.shape[2]):\n",
    "        ch = np.histogram(image[:,:,channel], bins=numBins, range=binsRange)\n",
    "        features = np.concatenate(ch)\n",
    "    return features\n",
    "\n",
    "\n",
    "# Given an image, generate HOG feature vector for the image with the following chosen parameters\n",
    "# 9 orientation, 8x8 pixels per cell, 2x2 cells per block\n",
    "# image - single channel image\n",
    "# bVisual - If True, also return an image of the HOG\n",
    "# bFeatVec - If True, return the data as a feature vector by calling .ravel() on the result just before returning.\n",
    "def get_hog_features(image, bVisual=False, bFeatVec=True):\n",
    "    # if bVisual == True, return features, hog_image, else just return features\n",
    "    return hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "               transform_sqrt=False, visualise=bVisual, feature_vector=bFeatVec)\n",
    "\n",
    "# Create a feature vector for an image. The image must be RGB and all channels of the image are used\n",
    "def extract_features(image):\n",
    "    featureImage = convert_color(image) # apply color conversion\n",
    "    spatialFeatures = get_spatial_features(featureImage)\n",
    "    histFeatures = get_histogram_features(featureImage)\n",
    "    hogFeatures = [] # get HOG features for all channels\n",
    "    for channel in range(featureImage.shape[2]):\n",
    "        hogFeatures.append(get_hog_features(featureImage[:,:,channel], bVisual=False, bFeatVec=True))\n",
    "    hogFeatures = np.ravel(hogFeatures)\n",
    "    return np.concatenate((spatialFeatures, histFeatures, hogFeatures))\n",
    "\n",
    "# Create an array of feature vectors\n",
    "def extract_features_from_files(imageFileNames):\n",
    "    features = [] # Create a list to append feature vectors to\n",
    "    for file in imageFileNames: # Iterate through the list of images\n",
    "        image = read_image(file)        \n",
    "        features.append(extract_features(image))\n",
    "    return features # Return list of feature vectors\n",
    "\n",
    "# Test feature extractor on a single channel image\n",
    "index = np.random.randint(0, len(carImageNames))\n",
    "image = read_image(carImageNames[index])\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "# Call our function with vis=True to see an image output\n",
    "hogFeatures, hogImage = get_hog_features(gray, bVisual=True, bFeatVec=False)\n",
    "features = extract_features(image)\n",
    "print('Feature vector length:',len(features))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('Example Car Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(hogImage, cmap='gray')\n",
    "plt.title('HOG Visualization')\n",
    "plt.show()\n",
    "\n",
    "# free up variables after use. I have limited resources on my machine\n",
    "del features, hogFeatures, hogImage, image, gray\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Training - Support Vector Machine\n",
    "\n",
    "Why use SVM and not something else?\n",
    "\n",
    "Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# Classifier training - prepare training and test datasets\n",
    "\n",
    "import time\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# need scikit-learn version to determine how to import train_test_split\n",
    "(skl_major, skl_minor, _) = sklearn.__version__.split(\".\")\n",
    "\n",
    "if int(skl_major) == 0 and int(skl_minor) <= 17:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "if int(skl_major) == 0 and int(skl_minor) > 17:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pick a random subset of the training set to train the classifier on, due to\n",
    "# computational constraints I have on my laptop.\n",
    "sampleSize = 6000\n",
    "carImageNamesSet = random.sample(carImageNames, sampleSize)\n",
    "notCarImageNamesSet = random.sample(notCarImageNames, sampleSize)\n",
    "\n",
    "# Extract features from training images\n",
    "t=time.time()\n",
    "carFeatures = extract_features_from_files(carImageNamesSet)\n",
    "notCarFeatures = extract_features_from_files(notCarImageNamesSet)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to extract features from', len(carFeatures)+len(notCarFeatures), 'training samples.')\n",
    "\n",
    "# Create an array stack of feature vectors \n",
    "featureStack = np.vstack((carFeatures, notCarFeatures)).astype(np.float64)                        \n",
    "featureScaler = StandardScaler().fit(featureStack) # Fit a per-column scaler\n",
    "scaledFeatureStack = featureScaler.transform(featureStack) # Apply the scaler\n",
    "\n",
    "# Define the labels vector\n",
    "labelStack = np.hstack((np.ones(len(carFeatures)), np.zeros(len(notCarFeatures))))\n",
    "\n",
    "# plot a randomly selected feature vector\n",
    "indx = np.random.randint(0, len(featureStack))\n",
    "fig = plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.plot(featureStack[indx])\n",
    "plt.title('Raw feature vector')\n",
    "plt.subplot(122)\n",
    "plt.plot(scaledFeatureStack[indx])\n",
    "plt.title('normalised feature vector')\n",
    "plt.show()\n",
    "\n",
    "# Split up training data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaledFeatureStack, labelStack, \n",
    "                                                    test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Number of images in the training set:', len(X_train))\n",
    "print('Number of images in the test set:', len(X_test))\n",
    "\n",
    "# free up variables after use. I have limited resources on my machine\n",
    "del carImageNames, notCarImageNames, carImageNamesSet, notCarImageNamesSet\n",
    "del carFeatures, notCarFeatures, featureStack\n",
    "del scaledFeatureStack, labelStack\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# Classifier training - do the training\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Create Linear Support Vector Machine and use it as the classifier \n",
    "svc = LinearSVC()\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "\n",
    "# Check the prediction time \n",
    "t=time.time()\n",
    "n_predict = 10\n",
    "print('My SVC predicts: ', svc.predict(X_test[0:n_predict]))\n",
    "print('For these',n_predict, 'labels: ', y_test[0:n_predict])\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')\n",
    "\n",
    "# free up variables after use. I have limited resources on my machine\n",
    "del X_train, X_test, y_train, y_test\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle Detection - Sliding Window Search\n",
    "Describe how (and identify where in your code) you implemented a sliding window search. How did you decide what scales to search and how much to overlap windows?\n",
    "Show some examples of test images to demonstrate how your pipeline is working. How did you optimize the performance of your classifier?\n",
    "\n",
    "__NOTE__: on selection of number of scales to search - scale=0.5 caused a lot of false positives at perspectively far distances from the car as well as nearby. At far distances we don't care about other vehicles. We do care about false positives nearby as that will cause an unwanted avoidance action that may lead to accidents. Therefore, false positives must be eliminated in the near vicinity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# Vehicle detection - sub-sampling window search\n",
    "\n",
    "# Find cars in a sub-region of the image and return corresponding bounding boxes\n",
    "def find_cars(image, ystart, ystop, scale, svc, X_scaler):\n",
    "    \n",
    "    # we will accumulate a list of potential vehicle locations here in the form [((x1, y1), (x2, y2)),...]\n",
    "    bboxes = []\n",
    "    \n",
    "    # Take the specified ROI in the image and transform the color space\n",
    "    subImage = image[ystart:ystop,:,:]\n",
    "    colorTransImage = convert_color(subImage)\n",
    "    \n",
    "    # re-scale image to find vehicles at scale specified\n",
    "    if scale != 1:\n",
    "        imshape = colorTransImage.shape\n",
    "        colorTransImage = cv2.resize(colorTransImage, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "    \n",
    "    # break the image into channels\n",
    "    ch1 = colorTransImage[:,:,0]\n",
    "    ch2 = colorTransImage[:,:,1]\n",
    "    ch3 = colorTransImage[:,:,2]\n",
    "\n",
    "    # pre-defined constants\n",
    "    pixelsPerCell = 8\n",
    "    cellsPerBlock = 2\n",
    "    numOrientations = 9\n",
    "    \n",
    "    # Define blocks and steps as above\n",
    "    numBlocksX = (ch1.shape[1] // pixelsPerCell)-1\n",
    "    numBlocksY = (ch1.shape[0] // pixelsPerCell)-1 \n",
    "    \n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    numBlocksPerWindow = (window // pixelsPerCell)-1\n",
    "    \n",
    "    cellsPerStep = 2  # define how many cells to step across as we slide the window\n",
    "    numStepsX = (numBlocksX - numBlocksPerWindow) // cellsPerStep\n",
    "    numStepsY = (numBlocksY - numBlocksPerWindow) // cellsPerStep\n",
    "       \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, bFeatVec=False)\n",
    "    hog2 = get_hog_features(ch2, bFeatVec=False)\n",
    "    hog3 = get_hog_features(ch3, bFeatVec=False)\n",
    "    \n",
    "    # slide window across and calculate the feature vector as we go\n",
    "    for xb in range(numStepsX):\n",
    "        for yb in range(numStepsY):\n",
    "            yPos = yb * cellsPerStep\n",
    "            xPos = xb * cellsPerStep\n",
    "            \n",
    "            # Extract HOG for this patch\n",
    "            hogFeatures1 = hog1[yPos:yPos+numBlocksPerWindow, xPos:xPos+numBlocksPerWindow].ravel() \n",
    "            hogFeatures2 = hog2[yPos:yPos+numBlocksPerWindow, xPos:xPos+numBlocksPerWindow].ravel() \n",
    "            hogFeatures3 = hog3[yPos:yPos+numBlocksPerWindow, xPos:xPos+numBlocksPerWindow].ravel() \n",
    "            hogFeatures = np.hstack((hogFeatures1, hogFeatures2, hogFeatures3))\n",
    "\n",
    "            xLeft = xPos * pixelsPerCell\n",
    "            yTop = yPos * pixelsPerCell\n",
    "\n",
    "            # Extract the image patch\n",
    "            patchImage = cv2.resize(colorTransImage[yTop:yTop+window, xLeft:xLeft+window], (64,64))\n",
    "          \n",
    "            # Get color features\n",
    "            spatialFeatures = get_spatial_features(patchImage)\n",
    "            histogramFeatures = get_histogram_features(patchImage)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            testFeatures = X_scaler.transform(\n",
    "                np.hstack((spatialFeatures, histogramFeatures, hogFeatures)).reshape(1, -1))    \n",
    "            testPrediction = svc.predict(testFeatures)\n",
    "            \n",
    "            # if we found a car, add the bounding box to the list\n",
    "            if testPrediction == 1:\n",
    "                xBoxLeft = np.int(xLeft * scale)\n",
    "                yBoxTop = np.int(yTop * scale)\n",
    "                winSize = np.int(window * scale)\n",
    "                bbox = ((xBoxLeft, yBoxTop+ystart),(xBoxLeft+winSize,yBoxTop+winSize+ystart)) \n",
    "                bboxes.append(bbox)\n",
    "                \n",
    "    return bboxes\n",
    "\n",
    "# Test it\n",
    "image = read_image('./test_images/test5.jpg')\n",
    "yStart = 400\n",
    "yStops = [550,600,650]\n",
    "scales = [1,2,4]\n",
    "bboxes = []\n",
    "for (scale,yStop) in zip(scales,yStops):\n",
    "    bboxes += find_cars(image, yStart, yStop, scale, svc, featureScaler)\n",
    "\n",
    "for bbox in bboxes:\n",
    "    cv2.rectangle(image, bbox[0], bbox[1], (0,0,255), 6)\n",
    "plt.imshow(image)\n",
    "plt.title('Vehicle detection on a single image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# Heatmap generation\n",
    "\n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "# Create a heatmap given potentially overlapping bounding boxes\n",
    "def create_heatmap(width, height, boundingBoxesList):\n",
    "    heatImage = np.zeros((width,height)).astype(np.float)\n",
    "    for box in boundingBoxesList:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatImage[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "    return heatImage\n",
    "\n",
    "# Threshold the heatmap\n",
    "def threshold_heatmap(heatmap, threshold):\n",
    "    heatmap[heatmap <= threshold] = 0 # Zero out pixels below the threshold\n",
    "    return heatmap\n",
    "\n",
    "def draw_labeled_bboxes(image, labels, colors):\n",
    "    for carNumber in range(1, labels[1]+1):\n",
    "        nonzero = (labels[0] == carNumber).nonzero() # Find pixels with each carNumber label value\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(image, bbox[0], bbox[1], colors[carNumber], 6)\n",
    "    # Return the image\n",
    "    return image\n",
    "\n",
    "# Test above methods\n",
    "\n",
    "# Create the heatmap\n",
    "heatImage = create_heatmap(image.shape[0], image.shape[1], bboxes) # create heat image\n",
    "heatImage = threshold_heatmap(heatImage,1) # Apply threshold to help remove false positives\n",
    "heatImage = np.clip(heatImage, 0, 255) # Clip to visualizable limits\n",
    "\n",
    "# Find final boxes from heatmap using label function\n",
    "colors = [(255,0,0),(0,255,0),(255,255,0),(0,0,255),(255,0,255),(0,255,255)]\n",
    "labels = label(heatImage)\n",
    "labelledImage = draw_labeled_bboxes(np.copy(image), labels, colors)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(labelledImage)\n",
    "plt.title('Bounding Boxes')\n",
    "plt.subplot(122)\n",
    "plt.imshow(heatImage, cmap='hot')\n",
    "plt.title('Heat Map')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Pipeline\n",
    "Summarise the steps in the processing pipeline. \n",
    "\n",
    "Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes.\n",
    "\n",
    "Provide a link to your final video output. Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.)\n",
    "\n",
    "Combine vehicle detection with lane detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# Processing pipeline\n",
    "\n",
    "class VehicleDetector:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._prevBboxes = []\n",
    "        self._numFrames = 0\n",
    "        self._maxNumVehicles = 0\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        print('Number of frames processed:', self._numFrames)\n",
    "        print('Maximum number of vehicles counted in a frame:', self._maxNumVehicles)\n",
    "        \n",
    "    def process_image(self,image):\n",
    "\n",
    "        self._numFrames += 1\n",
    "        \n",
    "        # region of the image we will search for vehicles, and the scales we will search at\n",
    "        yStart = 400\n",
    "        yStops = [550,600,650]\n",
    "        scales = [1,2,4]\n",
    "\n",
    "        # find bounding boxes enclosing vehicles at all scales\n",
    "        bboxes = []\n",
    "        for scale,yStop in zip(scales,yStops):\n",
    "            bboxes += find_cars(image, yStart, yStop, scale, svc, featureScaler)\n",
    "\n",
    "        # add list of bounding boxes from previous image(or last n images)\n",
    "        cumBboxes = bboxes + self._prevBboxes\n",
    "        self._prevBboxes = bboxes;\n",
    "        \n",
    "        # generate heatmap for combined list of bounding boxes\n",
    "        minDetections = 1 # Apply threshold to help remove false positives\n",
    "        heatImage = create_heatmap(image.shape[0], image.shape[1], cumBboxes)        \n",
    "        heatImage = threshold_heatmap(heatImage, minDetections)\n",
    "        heatImage = np.clip(heatImage, 0, 255)\n",
    "\n",
    "        # Find final boxes from heatmap using label function\n",
    "        labels = label(heatImage)\n",
    "        colors = [(127,127,127),(255,0,0),(0,255,0),(255,255,0),(0,0,255),(255,0,255),(0,255,255),(255,255,255)]\n",
    "        labelledImage = draw_labeled_bboxes(np.copy(image), labels, colors)\n",
    "        if labels[1] > self._maxNumVehicles:\n",
    "            self._maxNumVehicles = labels[1]\n",
    "        \n",
    "        return labelledImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection - Project Video\n",
    "All that remains now is to process the video sequence by applying the processing pipeline described in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# process the project video\n",
    "\n",
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "# set input and output\n",
    "output_video_name = 'output.mp4'\n",
    "input_video = VideoFileClip(\"test_video.mp4\")\n",
    "\n",
    "# Create the vehicle detector\n",
    "processor = VehicleDetector()\n",
    "process_image = processor.process_image\n",
    "\n",
    "# process video frame by frame\n",
    "output_video = input_video.fl_image(process_image)\n",
    "%time output_video.write_videofile(output_video_name, audio=False)\n",
    "\n",
    "# print some stats\n",
    "processor.get_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
